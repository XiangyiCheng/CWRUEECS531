{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Xiangyi Cheng (xxc273)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, a baseline and a deep neural network are required to be constructed. A dataset needs to be trained based on this neural network. The performance trained from the baseline and the deep neural network should be compared and discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is an artificial neural network with one or more hidden layers between the input and output layers which is inspired by biological neural networks that constitude animal brains. For the simplest case that only one layer exists (the diagram is shown below). Input is a set of observations, each an N-dimension vector. Weights are assigned to generate the hidden layer from the input layer. After obtaining the hidden layer, new weights are calculated to get the output layer. Specifically saying, the hidden layer is made of nodes. A node combines input from the data with a set of weights which called net input function. Then the result is passed through a node called activation function to determine if and how the signal progresses affect the output. In this case, weights are fully connected which means each node owns its weights from an input.\n",
    "![caption](nn.png)\n",
    "\n",
    "Extend this basic idea to a deep neural network which includes multiple layers. Once the first layer is obtained, the weights are assigned again to generate a new layer. After several times, the network consists of multiple layers which causes more accurate ultimate output. The diagram below illuminates a three-hidden-layer neural network.\n",
    "![caption](dnn.png)\n",
    "\n",
    "The weights mentioned above are the key to get a good model with accurate predictions. Propper weights are obtained by \"back progagation algorithm\" which was imbedded into many packages. And the common method of optimization algorithm is \"gradient descent\". Applying this algorithm, optimal weights are chosen to minimize the errors.\n",
    "\n",
    "The applications of deep neural network are broad and play an important role in several fields such as object detection and recognition, automatic speech recognition, visual art processing, etc.. The packages developed based on neural network are mature and various. In this assignment, TensorFlow is used to construct a deep neural network and train it on mnist dataset which is a large database of handwritten digits included 60000 training images and 10000 test images. The accuracy will be obtained after applying the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is an open-source software library used for machine learning applications such as neural networks. Basically, we will use TensorFlow to train the mnist database based on the models we build. The accuracy will be computated and printed out. To achieve this goal, TensorFlow and mnist database should be imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mnist and extract the data and label seperately. Class number is 10 due to 10 digit numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('/tmp/data/',one_hot=True)\n",
    "\n",
    "\n",
    "mnist_data=tf.placeholder('float',[None,784])\n",
    "label=tf.placeholder('float')\n",
    "\n",
    "# 10 different digit numbers: 0-9\n",
    "n_classes=10\n",
    "n_each_group=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Construction\n",
    "A model include only one hidden layer is constructed. The compulational idea is:\n",
    "$$ Hidden Layer = Input Data * weight + bias $$\n",
    "$$ Output Layer = Hidden Layer * weight + bias $$\n",
    "\n",
    "The nodes in hidden layer is set to 500 which could offer us a good result without consuming heavy computations. The input value is 784 since each image in mnist is in 28*28 dimenions. An activation function is called after obtaining the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_neural_network(data):\n",
    "\t# input_data * weight + bias\n",
    "\t# the \n",
    "\tn_hidden1=500\n",
    "\n",
    "\t# each image has 784 pixels which is calculated by 28*28.\n",
    "\thidden_layer1={'weights':tf.Variable(tf.random_normal([784,n_hidden1])),'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "\n",
    "\toutput_layer={'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\tlayer1 = tf.add(tf.matmul(data,hidden_layer1['weights']),hidden_layer1['biases'])\n",
    "\tact_layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "\tmodel = tf.matmul(act_layer1,output_layer['weights'])+output_layer['biases']\n",
    "\n",
    "\treturn model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is defined as below. The baseline neural network model we built above is passed into the training function. In the code, tf.train.AdamOptimizer().minimize() is applied to optimize the weights by reducing the errors. To view the training process, 10 epoches should be printed out to show the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(models):\n",
    "\n",
    "\tprep_model=baseline_neural_network(models)\n",
    "\tcost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prep_model, labels=label))\n",
    "\topitimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\tstep_epochs=10\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t\tfor epoch in range(step_epochs):\n",
    "\t\t\tLoss=0\n",
    "\t\t\tfor _ in range(int(mnist.train.num_examples/n_each_group)):\n",
    "\t\t\t\tepoch_x,epoch_y = mnist.train.next_batch(n_each_group)\n",
    "\t\t\t\t_,loss = sess.run([opitimizer,cost],feed_dict={models:epoch_x,label:epoch_y})\n",
    "\t\t\t\tLoss+=loss\n",
    "\t\t\tprint 'Step',epoch,'is completed out of',step_epochs, '. Loss is ',Loss\n",
    "\n",
    "\t\tcorrect=tf.equal(tf.argmax(prep_model,1),tf.argmax(label,1))\n",
    "\t\taccuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "\t\tprint 'baseline accuracy:',accuracy.eval({models:mnist.test.images,label:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to pass our mnist data into the defined trainning function by simply calling the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 is completed out of 10 . Loss is  13619.7621614\n",
      "Step 1 is completed out of 10 . Loss is  3424.94012737\n",
      "Step 2 is completed out of 10 . Loss is  2286.27826652\n",
      "Step 3 is completed out of 10 . Loss is  1654.57457437\n",
      "Step 4 is completed out of 10 . Loss is  1238.81159588\n",
      "Step 5 is completed out of 10 . Loss is  946.172238963\n",
      "Step 6 is completed out of 10 . Loss is  735.818641031\n",
      "Step 7 is completed out of 10 . Loss is  568.784084868\n",
      "Step 8 is completed out of 10 . Loss is  452.778618537\n",
      "Step 9 is completed out of 10 . Loss is  354.861006843\n",
      "baseline accuracy: 0.948\n"
     ]
    }
   ],
   "source": [
    "train_baseline(mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result is shown above that the accuracy is 0.948 based on one-hidden-layer-model. \n",
    "\n",
    "## Deep Neural Network Construction\n",
    "To improve the training result, more hidden layers should be added to construct a more complex and deeper neural network. After trained by multiple layers, the accuracy should increase somehow. To test this concept, a eight-layer-neural-network is built. \n",
    "\n",
    "The nodes of each hidden layer are initialized to 500, 1000, 1500, 2000, 2500, 2500, 2500, 2500, respectively. The whole building is similar to baseline construction but adding more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_network(data):\n",
    "\t# input_data * weight + bias\n",
    "\tn_hidden1=500\n",
    "\tn_hidden2=1000\n",
    "\tn_hidden3=1500\n",
    "\tn_hidden4=2000\n",
    "\tn_hidden5=2500\n",
    "\tn_hidden6=2500\n",
    "\tn_hidden7=2500\n",
    "\tn_hidden8=2500\n",
    "\n",
    "\thidden_layer1={'weights':tf.Variable(tf.random_normal([784,n_hidden1])),'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "\thidden_layer2={'weights':tf.Variable(tf.random_normal([n_hidden1,n_hidden2])),'biases':tf.Variable(tf.random_normal([n_hidden2]))}\n",
    "\thidden_layer3={'weights':tf.Variable(tf.random_normal([n_hidden2,n_hidden3])),'biases':tf.Variable(tf.random_normal([n_hidden3]))}\n",
    "\thidden_layer4={'weights':tf.Variable(tf.random_normal([n_hidden3,n_hidden4])),'biases':tf.Variable(tf.random_normal([n_hidden4]))}\n",
    "\thidden_layer5={'weights':tf.Variable(tf.random_normal([n_hidden4,n_hidden5])),'biases':tf.Variable(tf.random_normal([n_hidden5]))}\n",
    "\thidden_layer6={'weights':tf.Variable(tf.random_normal([n_hidden5,n_hidden6])),'biases':tf.Variable(tf.random_normal([n_hidden6]))}\n",
    "\thidden_layer7={'weights':tf.Variable(tf.random_normal([n_hidden6,n_hidden7])),'biases':tf.Variable(tf.random_normal([n_hidden7]))}\n",
    "\thidden_layer8={'weights':tf.Variable(tf.random_normal([n_hidden7,n_hidden8])),'biases':tf.Variable(tf.random_normal([n_hidden8]))}\n",
    "\n",
    "\n",
    "\toutput_layer1={'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\toutput_layer2={'weights':tf.Variable(tf.random_normal([n_hidden2,n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\toutput_layer8={'weights':tf.Variable(tf.random_normal([n_hidden8,n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "\tlayer1 = tf.add(tf.matmul(data,hidden_layer1['weights']),hidden_layer1['biases'])\n",
    "\tact_layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "\tlayer2 = tf.add(tf.matmul(act_layer1,hidden_layer2['weights']),hidden_layer2['biases'])\n",
    "\tact_layer2 = tf.nn.relu(layer2)\t\n",
    "\n",
    "\tlayer3 = tf.add(tf.matmul(act_layer2,hidden_layer3['weights']),hidden_layer3['biases'])\n",
    "\tact_layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "\tlayer4 = tf.add(tf.matmul(act_layer3,hidden_layer4['weights']),hidden_layer4['biases'])\n",
    "\tact_layer4 = tf.nn.relu(layer4)\n",
    "\n",
    "\tlayer5 = tf.add(tf.matmul(act_layer4,hidden_layer5['weights']),hidden_layer5['biases'])\n",
    "\tact_layer5 = tf.nn.relu(layer5)\n",
    "\n",
    "\tlayer6 = tf.add(tf.matmul(act_layer5,hidden_layer6['weights']),hidden_layer6['biases'])\n",
    "\tact_layer6 = tf.nn.relu(layer6)\n",
    "\n",
    "\tlayer7 = tf.add(tf.matmul(act_layer6,hidden_layer7['weights']),hidden_layer7['biases'])\n",
    "\tact_layer7 = tf.nn.relu(layer7)\n",
    "\n",
    "\tlayer8 = tf.add(tf.matmul(act_layer7,hidden_layer8['weights']),hidden_layer8['biases'])\n",
    "\tact_layer8 = tf.nn.relu(layer8)\n",
    "\n",
    "\n",
    "\tmodel_1 = tf.matmul(act_layer1,output_layer1['weights'])+output_layer1['biases']\n",
    "\n",
    "\tmodel_2 = tf.matmul(act_layer2,output_layer2['weights'])+output_layer2['biases']\n",
    "\n",
    "\tmodel_8 = tf.matmul(act_layer8,output_layer8['weights'])+output_layer8['biases']\n",
    "\n",
    "\n",
    "\treturn model_1,model_2,model_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better comparison, three models with 1 hidden layer, 2 hidden layers and 8 hidden layers are extracted. Next step is to train the deep neural network. Although three models are trained seperately, they share the same weights and bias assigned in the deep_neural_network(data) function. So the results are comparable and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(models):\n",
    "\n",
    "\tprep_model1,prep_model2,prep_model8=deep_neural_network(models)\n",
    "\n",
    "\tcost1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prep_model1, labels=label))\n",
    "\topitimizer1=tf.train.AdamOptimizer().minimize(cost1)\n",
    "\n",
    "\tcost2=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prep_model2, labels=label))\n",
    "\topitimizer2=tf.train.AdamOptimizer().minimize(cost2)\n",
    "\n",
    "\tcost8=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prep_model8, labels=label))\n",
    "\topitimizer8=tf.train.AdamOptimizer().minimize(cost8)\n",
    "\n",
    "\tstep_epochs=10\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t\t# baseline\n",
    "\t\tfor epoch in range(step_epochs):\n",
    "\t\t\tLoss=0\n",
    "\t\t\tfor _ in range(int(mnist.train.num_examples/n_each_group)):\n",
    "\t\t\t\tepoch_x,epoch_y = mnist.train.next_batch(n_each_group)\n",
    "\t\t\t\t_,loss = sess.run([opitimizer1,cost1],feed_dict={models:epoch_x,label:epoch_y})\n",
    "\t\t\t\tLoss+=loss\n",
    "\n",
    "\t\tcorrect1=tf.equal(tf.argmax(prep_model1,1),tf.argmax(label,1))\n",
    "\t\taccuracy1=tf.reduce_mean(tf.cast(correct1,'float'))\n",
    "\t\tprint 'baseline accuracy:',accuracy1.eval({models:mnist.test.images,label:mnist.test.labels})\n",
    "\n",
    "\n",
    "\t\t# 2 layes\n",
    "\t\tfor epoch in range(step_epochs):\n",
    "\t\t\tLoss=0\n",
    "\t\t\tfor _ in range(int(mnist.train.num_examples/n_each_group)):\n",
    "\t\t\t\tepoch_x,epoch_y = mnist.train.next_batch(n_each_group)\n",
    "\t\t\t\t_,loss = sess.run([opitimizer2,cost2],feed_dict={models:epoch_x,label:epoch_y})\n",
    "\t\t\t\tLoss+=loss\n",
    "\n",
    "\t\tcorrect2=tf.equal(tf.argmax(prep_model2,1),tf.argmax(label,1))\n",
    "\t\taccuracy2=tf.reduce_mean(tf.cast(correct2,'float'))\n",
    "\t\tprint '2 layers accuracy:',accuracy2.eval({models:mnist.test.images,label:mnist.test.labels})\n",
    "\n",
    "\n",
    "\t\t# 8 layers\n",
    "\t\tfor epoch in range(step_epochs):\n",
    "\t\t\tLoss=0\n",
    "\t\t\tfor _ in range(int(mnist.train.num_examples/n_each_group)):\n",
    "\t\t\t\tepoch_x,epoch_y = mnist.train.next_batch(n_each_group)\n",
    "\t\t\t\t_,loss = sess.run([opitimizer8,cost8],feed_dict={models:epoch_x,label:epoch_y})\n",
    "\t\t\t\tLoss+=loss\n",
    "\t\t\tprint 'Step',epoch,'is completed out of',step_epochs,'. Loss is ',Loss\n",
    "\n",
    "\t\tcorrect8=tf.equal(tf.argmax(prep_model8,1),tf.argmax(label,1))\n",
    "\t\taccuracy8=tf.reduce_mean(tf.cast(correct8,'float'))\n",
    "\t\tprint '8 layers accuracy:',accuracy8.eval({models:mnist.test.images,label:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the same as training the baseline, we pass our mnist data into the defined trainning function by simply calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy: 0.9424\n",
      "2 layers accuracy: 0.9564\n",
      "Step 0 is completed out of 10 . Loss is  7.08814258316e+13\n",
      "Step 1 is completed out of 10 . Loss is  1.30684436299e+13\n",
      "Step 2 is completed out of 10 . Loss is  6.75410713926e+12\n",
      "Step 3 is completed out of 10 . Loss is  4.75541507501e+12\n",
      "Step 4 is completed out of 10 . Loss is  4.30675179694e+12\n",
      "Step 5 is completed out of 10 . Loss is  3.19045525925e+12\n",
      "Step 6 is completed out of 10 . Loss is  2.64274842516e+12\n",
      "Step 7 is completed out of 10 . Loss is  2.78216182691e+12\n",
      "Step 8 is completed out of 10 . Loss is  2.39636090822e+12\n",
      "Step 9 is completed out of 10 . Loss is  1.70913980897e+12\n",
      "8 layers accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the outcome computed above, the accuracy is increasing with more hidden layers although less than 0.01 accuracy improved from single layer to 2-layer model. However, the accuracy is improve from 0.9424 to 0.9637 when 8 layers are applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models we built above are all based on the traditional neural network with fully connected weights. Besides the traditional approach, improvements are achieved by modifying the models in different ways. The relatively popular ones are Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). As the exploration, I used CNN to construct a network and trained it with mnist dataset. \n",
    "\n",
    "Similar to the traditional neural network, a CNN consists of an input and an output layer, as well as multiple hidden layers. However, the hidden layers of a CNN typically includes convolutional layers, pooling layers, fully connected layers and normalization layers.\n",
    "\n",
    "To achieve the goal to constuct a CNN model, TensorFlow is still used due to its useful imbedded functions. A function called convolutional_neural_network(data) is defined by constructing a 2-hidden-layer-CNN. Firstly, weights and bias are specified. Then convolutions are done to the data and each hidden layer. Next, pool the values by picking the maximum among them. Lastly, activate the function to get the model. The basic computational idea is still: $$ Hidden Layer = Input Data * weight + bias $$\n",
    "$$ Output Layer = Hidden Layer * weight + bias $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(data):\n",
    "\n",
    "\tweights={'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "\t         'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),'out':tf.Variable(tf.random_normal([1024,n_classes]))}\n",
    "\tbiases={'b_conv1':tf.Variable(tf.random_normal([32])),'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "\t         'b_fc':tf.Variable(tf.random_normal([1024])),'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\tdata=tf.reshape(data,shape=[-1,28,28,1])\n",
    "\n",
    "\tconv1=tf.nn.conv2d(data,weights['W_conv1'],strides=[1,1,1,1],padding='SAME')\n",
    "\tconv1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "\tconv2=tf.nn.conv2d(conv1,weights['W_conv2'],strides=[1,1,1,1],padding='SAME')\n",
    "\tconv2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "\tfc=tf.reshape(conv2,[-1,7*7*64])\n",
    "\tfc=tf.nn.relu(tf.matmul(fc,weights['W_fc'])+biases['b_fc'])\n",
    "\n",
    "\tmodel_CNN=tf.matmul(fc,weights['out'])+biases['out']\n",
    "\n",
    "\treturn model_CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the CNN model, it is required to train the data into with CNN. The whole procedure is similar to the traditional neural network except for calling the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(models):\n",
    "\n",
    "\tprep_model=convolutional_neural_network(models)\n",
    "\tcost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prep_model, labels=label))\n",
    "\topitimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\tstep_epochs=10\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t\tfor epoch in range(step_epochs):\n",
    "\t\t\tLoss=0\n",
    "\t\t\tfor _ in range(int(mnist.train.num_examples/n_each_group)):\n",
    "\t\t\t\tepoch_x,epoch_y = mnist.train.next_batch(n_each_group)\n",
    "\t\t\t\t_,loss = sess.run([opitimizer,cost],feed_dict={models:epoch_x,label:epoch_y})\n",
    "\t\t\t\tLoss+=loss\n",
    "\t\t\tprint 'Step',epoch,'is completed out of',step_epochs, '. Loss is ',Loss\n",
    "\n",
    "\t\tcorrect=tf.equal(tf.argmax(prep_model,1),tf.argmax(label,1))\n",
    "\t\taccuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "\t\tprint 'CNN accuracy:',accuracy.eval({models:mnist.test.images,label:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the mnist dataset with the CNN model we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 is completed out of 10 . Loss is  2218924.89516\n",
      "Step 1 is completed out of 10 . Loss is  487783.264467\n",
      "Step 2 is completed out of 10 . Loss is  295234.748389\n",
      "Step 3 is completed out of 10 . Loss is  202000.191294\n",
      "Step 4 is completed out of 10 . Loss is  125114.0923\n",
      "Step 5 is completed out of 10 . Loss is  99300.5750742\n",
      "Step 6 is completed out of 10 . Loss is  72220.6646907\n",
      "Step 7 is completed out of 10 . Loss is  54282.5147519\n",
      "Step 8 is completed out of 10 . Loss is  45919.4728761\n",
      "Step 9 is completed out of 10 . Loss is  46508.69596\n",
      "CNN accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "train_CNN(mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the assignment requirements, a neural network with single hidden layer called baseline is constructed by TensorFlow. Although I made two baseline models, the discussion will be based on the second one whose accuracy is 0.9424 after passing the mnist datasat into the training function simply because it shares the same model with 2-layer and 8-layer model. The same model ensures the comparability is robust and and the comparison is meaningful. After adding one more hidden layer to the baseline, the accuracy is improved to 0.9564. The procedure was repeated until 8 hidden layers are obtained. The accuracy is 0.9637 with the 8-layer model. We could say that the accuracy is increasing with more hidden layers. However, more hidden layers also need more computations to support. It is critical for us to determine how many layers we will use by balancing the ultimate outcome and the computational costs. \n",
    "\n",
    "As an exploration, Convolutional Neural Network was applied to mnist using TensorFlow as well. To save the computational time, a 2-layer CNN model is constructed. The accuracy is surprisingly high, 0.9657. Compared this result to the 2-layer traditional neural network model outcome whose accuracy is 0.9564, the CNN is more efficient and more accurate than the result of 8-layer one. But the computational time of CNN is much longer than the traditional one since more steps such as convolution, pooling are needed when building the model. \n",
    "\n",
    "In general, the performances of neural network are outstanding. We could tell this based on the accuracy 0.9424 from the baseline within several seconds. And there are still spaces to improve the outcomes such as using multiple hidden layers and other neural network models, for instance, CNN and RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deeplearning4j.org/neuralnet-overview\n",
    "\n",
    "https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "https://www.deeplearningtrack.com/single-post/2017/07/09/Introduction-to-NEURAL-NETWORKS-Advantages-and-Applications\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
